

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Processing and theory &mdash; dronewq 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=8d563738"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="install.html" />
    <link rel="prev" title="DroneWQ" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            dronewq
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Processing and theory</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#removal-of-surface-reflected-light-lt-lsr-lw">Removal of surface reflected light (L<sub>T</sub> - L<sub>SR</sub> = L<sub>W</sub>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#normalizing-by-downwelling-irradiance-lw-ed-rrs">Normalizing by downwelling irradiance (L<sub>W</sub> / E<sub>d</sub> =  R<sub>rs</sub>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rrs-pixel-masking">R<sub>rs</sub> pixel masking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#water-quality-retrievals">Water quality retrievals</a></li>
<li class="toctree-l2"><a class="reference internal" href="#georeferencing-and-mapping">Georeferencing and mapping</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="primary_demo.html">Demonstration of <code class="docutils literal notranslate"><span class="pre">DroneWQ</span></code> functions and processing code</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html"><code class="docutils literal notranslate"><span class="pre">Paralelogram2D</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.are_points_within_vertices"><code class="docutils literal notranslate"><span class="pre">are_points_within_vertices()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.blackpixel_method"><code class="docutils literal notranslate"><span class="pre">blackpixel_method()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.chl_gitelson"><code class="docutils literal notranslate"><span class="pre">chl_gitelson()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.chl_hu"><code class="docutils literal notranslate"><span class="pre">chl_hu()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.chl_hu_ocx"><code class="docutils literal notranslate"><span class="pre">chl_hu_ocx()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.chl_ocx"><code class="docutils literal notranslate"><span class="pre">chl_ocx()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.compute_flight_lines"><code class="docutils literal notranslate"><span class="pre">compute_flight_lines()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.compute_lines"><code class="docutils literal notranslate"><span class="pre">compute_lines()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.dls_ed"><code class="docutils literal notranslate"><span class="pre">dls_ed()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.downsample"><code class="docutils literal notranslate"><span class="pre">downsample()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.euclidean_distance"><code class="docutils literal notranslate"><span class="pre">euclidean_distance()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.georeference"><code class="docutils literal notranslate"><span class="pre">georeference()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.get_center"><code class="docutils literal notranslate"><span class="pre">get_center()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.get_warp_matrix"><code class="docutils literal notranslate"><span class="pre">get_warp_matrix()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.hedley_method"><code class="docutils literal notranslate"><span class="pre">hedley_method()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.is_on_right_side"><code class="docutils literal notranslate"><span class="pre">is_on_right_side()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.is_point_within_vertices"><code class="docutils literal notranslate"><span class="pre">is_point_within_vertices()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.load_images"><code class="docutils literal notranslate"><span class="pre">load_images()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.load_img_fn_and_meta"><code class="docutils literal notranslate"><span class="pre">load_img_fn_and_meta()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.mobley_rho_method"><code class="docutils literal notranslate"><span class="pre">mobley_rho_method()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.mosaic"><code class="docutils literal notranslate"><span class="pre">mosaic()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.panel_ed"><code class="docutils literal notranslate"><span class="pre">panel_ed()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.plot_basemap"><code class="docutils literal notranslate"><span class="pre">plot_basemap()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.plot_georeferenced_data"><code class="docutils literal notranslate"><span class="pre">plot_georeferenced_data()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.process_micasense_images"><code class="docutils literal notranslate"><span class="pre">process_micasense_images()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.process_raw_to_rrs"><code class="docutils literal notranslate"><span class="pre">process_raw_to_rrs()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.retrieve_imgs_and_metadata"><code class="docutils literal notranslate"><span class="pre">retrieve_imgs_and_metadata()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.rrs_std_pixel_masking"><code class="docutils literal notranslate"><span class="pre">rrs_std_pixel_masking()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.rrs_threshold_pixel_masking"><code class="docutils literal notranslate"><span class="pre">rrs_threshold_pixel_masking()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.save_images"><code class="docutils literal notranslate"><span class="pre">save_images()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.save_wq_imgs"><code class="docutils literal notranslate"><span class="pre">save_wq_imgs()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.tsm_nechad"><code class="docutils literal notranslate"><span class="pre">tsm_nechad()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html#utils.write_metadata_csv"><code class="docutils literal notranslate"><span class="pre">write_metadata_csv()</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">dronewq</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Processing and theory</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/theory.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="processing-and-theory">
<h1>Processing and theory<a class="headerlink" href="#processing-and-theory" title="Link to this heading"></a></h1>
<p>Following the notation style and large body of research from the optical oceanography community, UAS can measure remote sensing reflectance (R<sub>rs</sub>) defined as:</p>
<div align="center">
Eq. 1&nbsp;&nbsp;&nbsp;&nbsp; R<sub>rs</sub> (θ, φ, λ) = L<sub>W</sub>(θ, φ, λ) / E<sub>d</sub>(θ, φ, λ) 
</div>
<br/>
<p>where L<sub>W</sub> (W m<sup>-2</sup> nm<sup>-1</sup> sr<sup>-1</sup>) is water-leaving radiance, E<sub>d</sub> (W m<sup>-2</sup> nm<sup>-1</sup>) is downwelling irradiance, θ represents the sensor viewing angle between the sun and the vertical (zenith), φ represents the angular direction relative to the sun (azimuth) and λ represents wavelength.</p>
<p>Like all above-water optical measurements, UAS do not measure R<sub>rs</sub> directly as the at-sensor total radiance (L<sub>T</sub>, W m<sup>-2</sup> nm<sup>-1</sup> sr<sup>-1</sup>) constitutes the sum of L<sub>W</sub> and incident radiance reflected off the sea surface into the detector’s field of view, referred to as surface reflected radiance (L<sub>SR</sub>). While there is in reality also some scattering of light off air molecules and aerosols we consider that minimal at typical UAS altitudes. L<sub>W</sub> is thus the radiance that emanates from the water and contains a spectral shape and magnitude governed by optically active water constituents interacting with downwelling irradiance, while L<sub>SR</sub> is independent of water constituents and is instead governed by a given sea-state surface reflecting the downwelling light; a familiar example is sun glint. Here we define UAS total reflectance (R<sub>UAS</sub>) as:</p>
<div align="center">
Eq. 2&nbsp;&nbsp;&nbsp;&nbsp; R<sub>UAS</sub>(θ, Φ, λ) = L<sub>T</sub>(θ, Φ, λ) / E<sub>d</sub>(λ)
<br/><br/>
</div>
where
<br/><br/>
<div align="center">
Eq. 3&nbsp;&nbsp;&nbsp;&nbsp; L<sub>T</sub>(θ, Φ, λ) = L<sub>W</sub>(θ, Φ, λ) + L<sub>SR</sub>(θ, Φ, λ)
</div>
<br/>
<p>If the water surface was perfectly flat, incident light would reflect specularly and could be measured with known viewing geometries. This specular reflection of a level surface is known as the Fresnel reflection; however, most water bodies are not flat as winds and currents create tilting surface wave facets. Due to the differing orientation of wave facets reflecting radiance from different parts of the sky, L<sub>SR</sub> can vary widely within a single UAS image. A common approach to model L<sub>SR</sub> is to express it as the product of sky radiance (L<sub>sky</sub>, W m<sup>-2</sup> nm<sup>-1</sup> sr<sup>-1</sup>) and ρ, the effective sea-surface reflectance of the wave facet (<a class="reference external" href="https://doi.org/10.1364/AO.38.007442">Mobley 1999</a>, <a class="reference external" href="https://doi.org/10.1364/OE.18.026313">Lee et al 2010</a>):</p>
<div align="center">
Eq. 4&nbsp;&nbsp;&nbsp;&nbsp; L<sub>SR</sub>(θ, Φ, λ)= ρ(θ, Φ, λ) ∗ L<sub>sky</sub>(θ', Φ, λ)
<br/><br/>
</div>
Where θ' is the mirror of θ (θ' = 180-θ). Rearranging Eqs. 3 Eqs. 4, ρ can be derived by:
<br/><br/>
<div align="center">
Eq. 5&nbsp;&nbsp;&nbsp;&nbsp; ρ(θ, Φ, λ) = (L<sub>T</sub>(θ, Φ, λ) − L<sub>W</sub>(θ, Φ, λ)) / L<sub>sky</sub>(θ', Φ, λ)
</div>
<br/>
Given measurements of L<sub>sky</sub>, an accurate determination of ⍴ is critical to derive R<sub>rs</sub> by:
<div align="center">
<br/>
Eq. 6&nbsp;&nbsp;&nbsp;&nbsp; R<sub>rs</sub>(θ, Φ, λ) = R<sub>UAS</sub>(θ, Φ, λ) − (L<sub>sky</sub>(θ', Φ, λ) ∗ ρ(θ, Φ, λ) / E<sub>d</sub>(λ))
</div>
<br/>
<p><code class="docutils literal notranslate"><span class="pre">DroneWQ</span></code> provides multiple options from the literature for removing L<sub>SR</sub>.</p>
<p>A secondary challenge with aquatic UAS remote sensing is georferencing and mosaicking imagery. Many UAS remote sensing studies use Structure-from-Motion (SfM) photogrammetric techniques to stitch individual UAS images into ortho- and georectified mosaics. Current photogrammetry techniques are not capable of stitching UAS images captured over large bodies of water due to a lack of key points in images of homogenous water surfaces. <br></p>
<p>The main processing script has the ability to <strong>1)</strong> convert raw multispectral imagery to total radiance (L<sub>t</sub>) with units of W/m<sup>2</sup>/nm/sr, <strong>2)</strong> remove surface reflected light (L<sub>sr</sub>) to calculate water leaving radiance (L<sub>w</sub>), <strong>3)</strong> measure downwelling irradiance (E<sub>d</sub>) from either the calibrated reflectance panel, downwelling light sensor (DLS), or a combination of the two, <strong>4)</strong> calculate remote sensing reflectance (R<sub>rs</sub>) by dividing E<sub>d</sub> by L<sub>w</sub>, and <strong>5)</strong> mask pixels containing specular sun glint or instances of vegetation, shadowing, etc., <strong>6)</strong> use R<sub>rs</sub> as input into various bio-optical algorithms to derive chlorophyll a and total suspended sediment concentrations, and <strong>7)</strong> georeference using image metadata and sensor specifications to orient and map to a known coordinate system.</p>
<section id="removal-of-surface-reflected-light-lt-lsr-lw">
<h2>Removal of surface reflected light (L<sub>T</sub> - L<sub>SR</sub> = L<sub>W</sub>)<a class="headerlink" href="#removal-of-surface-reflected-light-lt-lsr-lw" title="Link to this heading"></a></h2>
<p>The inclusion of L<sub>SR</sub> can lead to an overestimation of R<sub>rs</sub> and remotely sensed water quality retrievals, as shown in Figure 1. <code class="docutils literal notranslate"><span class="pre">DroneWQ</span></code> provides three common approaches to remove L<sub>SR</sub> as described below. An intercomparison of L<sub>SR</sub> removal methods can be found in <a class="reference external" href="https://doi.org/10.3389/fenvs.2021.674247">Windle &amp; Silsbe (2021)</a>.</p>
<p><img alt="Caption for example figure.abel{fig:removal_Lsr_fig}" src="_images/removal_Lsr_fig.jpg" />
<br/>
Figure 1. Example of an individual UAS image (green band) with different radiometric values: (A) R<sub>UAS</sub>, (B) R<sub>UAS</sub> with initial sun glint masking and (C–F) remote sensing reflectance (R<sub>rs</sub>) using various methods to remove surface reflected light: (C) ⍴ look-up table (LUT) from HydroLight simulations, (D) Dark pixel assumption with NIR = 0, (E) Dark pixel assumption with NIR &gt; 0, (F) Deglingting methods following <a class="reference external" href="https://doi.org/10.1080/01431160500034086">Hedley et al (2005)</a>. Figure taken from <a class="reference external" href="https://doi.org/10.3389/fenvs.2021.674247">Windle &amp; Silsbe (2021)</a>.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">DroneWQ</span></code>, we provide the following methods to calculate L<sub>w</sub>:</p>
<p><code class="docutils literal notranslate"><span class="pre">blackpixel_method()</span></code>
<br/>
One method to remove L<sub>SR</sub> relies on the so-called black pixel assumption that assumes L<sub>W</sub> in the near infrared (NIR) is negligible due to strong absorption of water. Where this assumption holds, at-sensor radiance measured in the NIR is solely L<sub>SR</sub> and allows ⍴ to be calculated if L<sub>sky</sub> is known. Studies have used this assumption to estimate and remove L<sub>SR</sub>; however, the assumption tends to fail in more turbid waters where high concentrations of particles enhance backscattering and L<sub>W</sub> in the NIR <a class="reference external" href="https://doi.org/10.1364/AO.39.003582">(Siegel et al 2000)</a>. <em>Therefore, this method should only be used in waters whose optical propeties are dominated and co-vary with phytoplankton (e.g. Case 1, open ocean waters).</em></p>
<p><code class="docutils literal notranslate"><span class="pre">mobley_rho_method()</span></code>
<br/>
Tabulated ρ values have been derived from numerical simulations with modelled sea surfaces, Cox and Munk wave states (wind), and viewing geometries (<a class="reference external" href="https://doi.org/10.1364/AO.38.007442">Mobley 1999</a>). Mobley (1999) provides the recommendation of collecting radiance measurements at viewing directions of θ = 40° from nadir and ɸ = 135° from the sun to minimize the effects of sun glint and nonuniform sky radiance with a ⍴ value of 0.028 for wind speeds less than 5 m/s. These suggested viewing geometries and ⍴ value have been used to estimate and remove L<sub>SR</sub> in many remote sensing studies. <em>This method should only be used if using a UAS sensor that is angled 30-40° from nadir throughout the flight and if wind speed is less than 5 m/s.</em></p>
<p><code class="docutils literal notranslate"><span class="pre">hedley_method()</span></code>
<br/>
Other methods to remove L<sub>SR</sub> include modelling a constant ‘ambient’ NIR signal that is removed from all pixels. This method relies on two assumptions: 1) The brightness in the NIR is composed only of sun glint and a spatially constant ‘ambient’ NIR component, and 2) The amount of L<sub>SR</sub> in the visible bands is linearly related to the amount in the NIR band <a class="reference external" href="https://doi.org/10.1080/01431160500034086">(Hedley et al 2005)</a>. Briefly, the minimum 10% of NIR radiance, min(Lt<sub>NIR</sub>), is calculated from a random subset of images. Next, linear relationships are established between the Lt<sub>NIR</sub> and the visible band values, which would be homogenous if not for the presence of L<sub>SR</sub>. Then, the slope (<em>b</em>) of the regressions are used to predict L<sub>SR</sub> for all pixels in the visible bands that would be expected if those pixels had a Lt<sub>NIR</sub> value of min(Lt<sub>NIR</sub>):</p>
<div align="center">
<br/>
Lw<sub>i</sub> = Lt<sub>i</sub> - b<sub>i</sub>(Lt(NIR) - min(Lt<sub>NIR</sub>)), where i is each band
<br/>
</div>
<br/>
<p><em>This method can be utilized without the collection of L<sub>sky</sub> images.</em></p>
</section>
<section id="normalizing-by-downwelling-irradiance-lw-ed-rrs">
<h2>Normalizing by downwelling irradiance (L<sub>W</sub> / E<sub>d</sub> =  R<sub>rs</sub>)<a class="headerlink" href="#normalizing-by-downwelling-irradiance-lw-ed-rrs" title="Link to this heading"></a></h2>
<p>After L<sub>SR</sub> is removed from L<sub>T</sub>, the product of that removal (L<sub>W</sub>) needs to be normalized by E<sub>d</sub> to calculate R<sub>rs</sub> (Eq. 6). The downwelling light sensor (DLS) or calibration reflectance panel can be used to calculate E<sub>d</sub>.</p>
<p>The following are methods to retrieve E<sub>d</sub>: <br>
<code class="docutils literal notranslate"><span class="pre">panel_ed()</span></code>
<br/>
An image capture of the MicaSense calibrated reflectance panel with known reflectance values can be used to calculate E<sub>d</sub>. It is recommended to use this method when flying on a clear sunny day.</p>
<p><code class="docutils literal notranslate"><span class="pre">dls_ed()</span></code>
<br/>
The MicaSense DLS measures downwelling hemispherical irradiance (E<sub>d</sub>) in the same spectral wavebands during in-flight image captures. According to MicaSense, the DLS is better at estimating changing light conditions (e.g. variable cloud cover) since it records DLS throughout a flight; however, it is not a perfect measurement due to movement of the UAS. The the MicaSense function <code class="docutils literal notranslate"><span class="pre">capture.dls_irradiance()</span></code> incorporates tilt-compensated DLS values from the onboard orientation sensor but is imperfect.</p>
<p>On days with changing cloud conditions it is recommended to use both the DLS and calibration reflectance panel measurements, when possible. This is done by applying a compensation factor from the calibration reflectance panel to all DLS measurements. This can be done by setting the argument dls_corr to TRUE in <code class="docutils literal notranslate"><span class="pre">dls_ed()</span></code>.</p>
<br/> 
</section>
<section id="rrs-pixel-masking">
<h2>R<sub>rs</sub> pixel masking<a class="headerlink" href="#rrs-pixel-masking" title="Link to this heading"></a></h2>
<p>An optional pixel masking procedure can be applied to R<sub>rs</sub> data to remove instances of specular sun glint and other artifacts in the imagery such as adjacent land, vegetation shadowing, or boats when present in the imagery. Pixels can be masked two ways:</p>
<p><code class="docutils literal notranslate"><span class="pre">rrs_threshold_pixel_masking()</span></code>
<br/>
This function masks pixels based on a user supplied R<sub>rs</sub> thresholds to mask pixels containing values &gt; R<sub>rs</sub>(NIR) threshold and &lt; R<sub>rs</sub>(green) threshold.</p>
<p><code class="docutils literal notranslate"><span class="pre">rrs_std_pixel_masking()</span></code>
<br/>
This function masks pixels based on a user supplied NIR factor. The mean and standard deviation of NIR is calculated from a user supplied amount of images, and pixels contain a NIR value &gt; mean + std * mask_std_factor are masked. The lower the mask_std_factor, the more pixels will be masked.
<br/></p>
</section>
<section id="water-quality-retrievals">
<h2>Water quality retrievals<a class="headerlink" href="#water-quality-retrievals" title="Link to this heading"></a></h2>
<p>R<sub>rs</sub> is often used as input into various bio-optical algorithms to obtain concentrations of optically active water quality constituents such as chlorophyll-a or total suspended matter (TSM). Several functions can be applied to calculate concentrations.</p>
<p><code class="docutils literal notranslate"><span class="pre">chl_hu()</span></code>
<br/>
This is the Ocean Color Index (CI) three-band reflectance difference algorithm <a class="reference external" href="https://doi.org/10.1029/2011JC007395">(Hu et al 2012)</a>. This should only be used for waters where chlorophyll-a retrievals are expected to be below 0.15 mg m^-3.
<br/></p>
<p><code class="docutils literal notranslate"><span class="pre">chl_ocx()</span></code>
<br/>
This is the OCx algorithm which uses a fourth-order polynomial relationship <a class="reference external" href="https://doi.org/10.1029/98JC02160">(O’Reilly et al 1998)</a>. This should be used for chlorophyll retrievals above 0.2 mg m^-3. The coefficients for OC2 (OLI/Landsat 8) are used as default as the closest match in bands to the Micasense sensors.
<br/></p>
<p><code class="docutils literal notranslate"><span class="pre">chl_hu_ocx()</span></code>
<br/>
This is the blended NASA chlorophyll algorithm which merges the Hu et al. (2012) color index (CI) algorithm (chl_hu) and the O’Reilly et al. (1998) band ratio OCx algortihm (chl_ocx). This specific code is grabbed from https://github.com/nasa/HyperInSPACE. Documentation can be found here https://www.earthdata.nasa.gov/apt/documents/chlor-a/v1.0.
<br/></p>
<p><code class="docutils literal notranslate"><span class="pre">chl_gitelson()</span></code>
<br/>
This algorithm estimates chlorophyll-a concentrations using a 2-band algorithm designed and recommended for turbid coastal (Case 2) waters <a class="reference external" href="https://doi.org/10.1016/j.rse.2007.01.016">(Gitelson et al 2007)</a>.
<br/></p>
<p><code class="docutils literal notranslate"><span class="pre">nechad_tsm()</span></code>
<br/>
This algorithm estimates total suspended matter (TSM) concentrations and is tuned and tested in turbid waters <a class="reference external" href="https://doi.org/10.1016/j.rse.2009.11.022">(Nechad et al 2010)</a>.
<br/></p>
</section>
<section id="georeferencing-and-mapping">
<h2>Georeferencing and mapping<a class="headerlink" href="#georeferencing-and-mapping" title="Link to this heading"></a></h2>
<p>Many UAS remote sensing studies use Structure-from-Motion (SfM) photogrammetric techniques to stitch individual UAS images into ortho- and georectified mosaics. This approach applies matching key points from overlapping UAS imagery in camera pose estimation algorithms to resolve 3D camera location and scene geometry. Commonly used SfM software (e.g. Pix4D, Agisoft Metashape) provide workflows that radiometrically calibrate, georeference, and stitch individual UAS images using a weighted average approach to create at-sensor reflectance 2D orthomosaics. Current photogrammetry techniques are not capable of stitching UAS images captured over large bodies of water due to a lack of key points in images of homogenous water surfaces. <br></p>
<p>In <code class="docutils literal notranslate"><span class="pre">DroneWQ</span></code>, we provide methods for georeferencing and mosaicking UAS imagery over water based on the “direct georeferencing” technique, which compensates for the absence of common features between UAS images by using specific aspects of the aircraft’s positioning during flight (latitude, longitude, altitude, and flight orientation), along with certain sensor characteristics (focal length, image size, sensor size, and focal plane dimensions).</p>
<p><code class="docutils literal notranslate"><span class="pre">georeference()</span></code>
<br/>
This function uses MicaSense metadata (altitude, pitch, roll, yaw, lat, lon) or user supplied data to georeference all captures to a known coordinate space. See notes on georeferencing below.</p>
<p><code class="docutils literal notranslate"><span class="pre">mosaic()</span></code>
<br/>
This function mosaics all the given georeferenced captures into one georeferenced mosaicked raster file.</p>
<p><code class="docutils literal notranslate"><span class="pre">downsample()</span></code>
<br/>
This function performs a downsampling procedure to reduce the spatial resolution of the final georeferenced mosaic.</p>
<p><code class="docutils literal notranslate"><span class="pre">plot_basemap()</span></code>
<br/>
This function loads a basemap and plots the georeferenced mosaic in the axes provides using pseudo-Mercator projection (epsg:3857).</p>
<p>Notes on georeferencing:</p>
<ul class="simple">
<li><p>The pitch, roll, and yaw angles are associated with the MicaSense sensor. The following statements should help you understand the angles:</p>
<ul>
<li><p>pitch = 0°, roll = 0°, yaw = 0° means: the sensor is nadir (looking down to the ground), the sensor is assumed to be fixed (or on a gimbal), and not moving side to side, the top of the image points to the north.</p></li>
<li><p>pitch = 90°, roll = 0°, yaw = 0° means: the sensor is looking forward from the aircraft, the sensor is assumed to be fixed (or on a gimbal), and not moving side to side, the top of the image points to the north.</p></li>
<li><p>pitch = 0°, roll = 0°, yaw = 90° means: the sensor is nadir (looking down to the ground), the sensor is assumed to be fixed (or on a gimbal), and not moving side to side, the top of the image points to the east.
<br/></p></li>
</ul>
</li>
<li><p>If possible, it is recommended to fly the UAS using a consistent yaw angle (e.g. UAS/sensor does not turn 180° every transect). This will make georeferencing easier and alleviate issues with changing sun glint on different transects. Some UAS flight planning softwares allow you to do this (e.g. <a class="reference external" href="https://www.sphengineering.com/flight-planning">UgCS</a>. If not, it is recommended that you note the UAS/sensor’s yaw angle and use this in the <code class="docutils literal notranslate"><span class="pre">georeference()</span></code> function.</p></li>
<li><p>If a yaw angle is not available, it is recommended to test a couple captures that contain land or the shoreline. The MicaSense sensor contains an Inertial Measurement Unit (IMU) that collects data on the sensor pitch, roll, and yaw; however, this data can be impacted by IMU errors, especially during turns and windy flights. You can see how much the sensor yaw angle varies by plotting the IMU metadata yaw angle over captures. An example is included in the primary_demo.ipynb.</p>
<ul>
<li><p>If you have a small dataset, you can manually go through the captures to select which ones line up with what transect to inform the yaw angle in the georeference() function. It is recommended to skip images that are taken when the UAS/sensor is turning since the IMU is prone to errors during UAS turns.</p></li>
<li><p>If you have a large dataset where this can be too time consuming, we have provided functions to automatically select captures with varying yaw angles that line up with different transects. The <code class="docutils literal notranslate"><span class="pre">compute_flight_lines()</span></code> function returns a list of image captures taken in different transects that contain consistent yaw angles. This can be incorporated into the  <code class="docutils literal notranslate"><span class="pre">georeference()</span></code> function to improve georeferencing and mosaicking.</p></li>
</ul>
</li>
</ul>
<p><img alt="Caption for example figure.abel{fig:chl_mosaic}" src="_images/chl_mosaic.png" />
<br/>
Figure 2. Final orthmosaic of UAS images collected over Western Lake Erie processed to chlorophyll a concentration.</p>
</section>
<section id="references">
<h2>References:<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p>Windle A.E. and Silsbe G.M., Evaluation of Unoccupied Aircraft System (UAS) Remote Sensing Reflectance Retrievals for Water Quality Monitoring in Coastal Waters. Front. Environ. Sci. 9:674247. (2021) doi: 10.3389/fenvs.2021.674247</p>
<p>Mobley C.D., Estimation of the remote-sensing reflectance from above-surface measurements, Appl. Opt. 38, 7442-7455 (1999) doi: 10.1364/AO.38.007442</p>
<p>Lee Z., Ahn Y., Mobley C.D., and Arnone R., Removal of surface-reflected light for the measurement of remote-sensing reflectance from an above-surface platform, Opt. Express 18, 26313-26324 (2010) doi: 10.1364/OE.18.026313</p>
<p>Hedley, J. D., Harborne, A. R., &amp; Mumby, P. J., Technical note: Simple and robust removal of sun glint for mapping shallow‐water benthos. International Journal of Remote Sensing, 26(10), 2107–2112. (2005) doi: 10.1080/01431160500034086</p>
<p>Siegel D.A., Wang M., Maritorena S., and Robinson W., Atmospheric correction of satellite ocean color imagery: the black pixel assumption, Appl. Opt. 39, 3582-3591 (2000) doi: 10.1364/AO.39.003582</p>
<p>Hu, C., Z. Lee, and B. Franz, Chlorophyll aalgorithms for oligotrophic oceans: A novel approach based on three-band reflectance difference, J. Geophys. Res., 117, C01011, (2012) doi: 10.1029/2011JC007395</p>
<p>O’Reilly, J. E., S. Maritorena, B. G. Mitchell, D. A. Siegel, K. L. Carder, S. A. Garver, M. Kahru, and C. McClain, Ocean color chlorophyll algorithms for SeaWiFS, J. Geophys. Res., 103(C11), 24937–24953, (1998) doi: 10.1029/98JC02160</p>
<p>Gitelson A.A., J. F. Schalles, C. M. Hladik, Remote chlorophyll-a retrieval in turbid, productive estuaries: Chesapeake Bay case study, Remote Sensing of Environment, 109(4), 464-472, (2007) doi: 10.1016/j.rse.2007.01.016.</p>
<p>Nechad B., K.G. Ruddick, Y. Park, Calibration and validation of a generic multisensor algorithm for mapping of total suspended matter in turbid waters, Remote Sensing of Environment, 114(4), 854-866, (2010) doi: 10.1016/j.rse.2009.11.022.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="DroneWQ" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="install.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Anna Windle, Patrick Gray.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>